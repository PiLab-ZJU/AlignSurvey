import json
import re
from collections import defaultdict
import jieba
import numpy as np
import pandas as pd
from datetime import datetime
import os
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, f1_score


def decode_unicode_escapes(obj):
    """é€’å½’è§£ç Unicodeè½¬ä¹‰å­—ç¬¦"""
    if isinstance(obj, str):
        def repl(m):
            return chr(int(m.group(1), 16))

        return re.sub(r'\\u([0-9a-fA-F]{4})', repl, obj)
    elif isinstance(obj, list):
        return [decode_unicode_escapes(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: decode_unicode_escapes(v) for k, v in obj.items()}
    else:
        return obj

def fix_invalid_unicode_escapes(text: str) -> str:
    """ä¿®å¤æ— æ•ˆçš„Unicodeè½¬ä¹‰åºåˆ—"""
    # \u åé¢ä¸æ˜¯ 4 ä½ 0-9a-f å°±æ›¿æ¢ä¸º \\u
    return re.sub(r'\\u(?![0-9a-fA-F]{4})', r'\\\\u', text)


def extract_content_after_think(text: str) -> str:
    """å¤„ç†<think>æ ‡ç­¾ï¼Œæå–</think>ä¹‹åçš„å†…å®¹"""
    # å…ˆè§£ç Unicodeè½¬ä¹‰å­—ç¬¦
    text = decode_unicode_escapes(text)

    # æ£€æŸ¥æ˜¯å¦åŒ…å«</think>æ ‡ç­¾
    think_end_pattern = r'</think>\s*\n\s*\n'
    match = re.search(think_end_pattern, text)

    if match:
        # æå–</think>\n\nä¹‹åçš„å†…å®¹
        content_after_think = text[match.end():]
        return content_after_think.strip()
    else:
        # å¦‚æœæ²¡æœ‰</think>æ ‡ç­¾ï¼Œè¿”å›åŸå†…å®¹
        return text.strip()


def clean_markdown_json(text: str) -> str:
    """æ¸…ç†markdownä»£ç å—æ ‡è®°"""
    text = re.sub(r'^```(json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```$', '', text)
    return text.strip()


def extract_json_from_text(text: str) -> dict:
    """ä»æ–‡æœ¬ä¸­æå–å¹¶è§£æJSON"""
    # 1. æ¸…ç†markdownæ ‡è®°

    # 2. ä¿®å¤æ— æ•ˆUnicodeè½¬ä¹‰
    cleaned = fix_invalid_unicode_escapes(text)

    # 3. è§£ç Unicodeè½¬ä¹‰å­—ç¬¦
    cleaned = decode_unicode_escapes(cleaned)
    cleaned = extract_content_after_think(cleaned)
    cleaned = clean_markdown_json(cleaned)

    try:
        # å°è¯•è§£æJSON
        parsed_json = json.loads(cleaned.strip())
        # å¤„ç†reasonå­—æ®µï¼ˆå¦‚æœæ˜¯åˆ—è¡¨åˆ™åˆå¹¶ä¸ºå­—ç¬¦ä¸²ï¼‰
        if isinstance(parsed_json.get("reason"), list):
            parsed_json["reason"] = " ".join(parsed_json["reason"])
        return parsed_json
    except json.JSONDecodeError:
        try:
            parsed_json = json.loads(cleaned + "\"}")
            if isinstance(parsed_json.get("reason"), list):
                parsed_json["reason"] = " ".join(parsed_json["reason"])
            return parsed_json
        except:
            print(f"JSONè§£æå¤±è´¥: {cleaned[:20]}...")
            return {"predict": "é”™è¯¯", "reason": " "}


def calculate_text_similarity(pred_reason: str, label_reason: str) -> dict:
    def clean_and_tokenize_chinese(text):
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ï¼Œä¿ç•™ä¸­æ–‡å’Œè‹±æ–‡
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        return set(jieba.cut(text))

    # è·å–è¯æ±‡é›†åˆ
    pred_tokens = clean_and_tokenize_chinese(pred_reason)
    label_tokens = clean_and_tokenize_chinese(label_reason)

    # è®¡ç®—è¯æ±‡é‡å 
    common_words = pred_tokens.intersection(label_tokens)
    vocab_overlap = len(common_words)

    # è®¡ç®—Jaccardç›¸ä¼¼åº¦
    union_words = pred_tokens.union(label_tokens)
    jaccard_sim = len(common_words) / len(union_words) if union_words else 0

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    vectorizer = CountVectorizer(token_pattern=r'(?u)\b\w+\b|[\u4e00-\u9fff]+')
    try:
        vectorizer.fit([pred_reason, label_reason])
        vectors = vectorizer.transform([pred_reason, label_reason])
        cosine_sim = cosine_similarity(vectors)[0, 1]
    except:
        cosine_sim = 0

    return {
        "vocabulary_overlap": vocab_overlap,
        "common_words": list(common_words),
        "jaccard_similarity": jaccard_sim,
        "cosine_similarity": cosine_sim
    }


def analyze_single_prediction(prediction_str: str, label_str: str) -> dict:
    """åˆ†æå•ä¸ªé¢„æµ‹ç»“æœ"""
    # æ­¥éª¤1: è§£æé¢„æµ‹ç»“æœJSON
    if "assistantfinal" in prediction_str:
        prediction_str = prediction_str.split("assistantfinal")[-1].strip()
    if "assistantfinal" in label_str:
        label_strs = label_str.split("analysisassistantfinal")[-1].strip()

    prediction_json = extract_json_from_text(prediction_str)
    prediction = prediction_json["predict"]
    prediction_reason = prediction_json["reason"]

    # æ­¥éª¤2: å¤„ç†æ ‡ç­¾ï¼ˆå…ˆå¤„ç†<think>æ ‡ç­¾ï¼Œå†è§£æJSONï¼‰
    processed_label = extract_content_after_think(label_str)
    label_json = extract_json_from_text(processed_label)
    label = label_json['predict']
    label_reason = label_json['reason']

    # æ­¥éª¤3: è®¡ç®—ç›¸ä¼¼åº¦
    similarities = calculate_text_similarity(prediction_reason, label_reason)

    # æ­¥éª¤4: åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
    is_correct = (prediction == label)

    return {
        'prediction': prediction,
        'prediction_reason': prediction_reason,
        'label': label,
        'label_reason': label_reason,
        'is_correct': is_correct,
        'similarities': similarities
    }


def analyze_predictions_file(file_path: str) -> dict:
    """åˆ†æå•ä¸ªé¢„æµ‹ç»“æœæ–‡ä»¶"""
    results = []
    total = 0
    correct = 0

    # ç”¨äºè®¡ç®—æ•´ä½“æŒ‡æ ‡
    all_predicted_labels = []
    all_true_labels = []

    # ç›¸ä¼¼åº¦ç´¯è®¡
    total_vocab_overlap = 0
    total_jaccard_sim = 0
    total_cosine_sim = 0

    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    categories = defaultdict(lambda: {
        'total': 0, 'correct': 0, 'errors': 0,
        'vocab_overlap': 0, 'jaccard_sim': 0, 'cosine_sim': 0,
        'error_details': []  # æ–°å¢ï¼šè®°å½•é”™è¯¯è¯¦æƒ…
    })

    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    category_true_positives = defaultdict(int)
    category_false_positives = defaultdict(int)
    category_false_negatives = defaultdict(int)
    all_categories = set()

    mismatches = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                record = json.loads(line)

                # åˆ†æå•ä¸ªé¢„æµ‹
                try:
                    analysis = analyze_single_prediction(record["predict"], record['label'])
                except:
                    analysis = analyze_single_prediction(record["predict"], record['output'])

                prediction = analysis['prediction']
                label = analysis['label']

                # æ·»åŠ åˆ°ç±»åˆ«é›†åˆ
                all_categories.add(label)
                all_categories.add(prediction)

                total += 1
                all_predicted_labels.append(prediction)
                all_true_labels.append(label)

                # ç´¯è®¡ç›¸ä¼¼åº¦æŒ‡æ ‡
                similarities = analysis['similarities']
                total_vocab_overlap += similarities['vocabulary_overlap']
                total_jaccard_sim += similarities['jaccard_similarity']
                total_cosine_sim += similarities['cosine_similarity']

                # æŒ‰ç±»åˆ«ç»Ÿè®¡
                categories[label]['total'] += 1
                categories[label]['vocab_overlap'] += similarities['vocabulary_overlap']
                categories[label]['jaccard_sim'] += similarities['jaccard_similarity']
                categories[label]['cosine_sim'] += similarities['cosine_similarity']

                if analysis['is_correct']:
                    correct += 1
                    categories[label]['correct'] += 1
                    category_true_positives[label] += 1
                else:
                    # é”™è¯¯ç»Ÿè®¡
                    categories[label]['errors'] += 1
                    categories[label]['error_details'].append({
                        'line_number': line_num,
                        'predicted_as': prediction,
                        'prediction_reason': analysis['prediction_reason'],
                        'label_reason': analysis['label_reason']
                    })

                    # åˆ†ç±»æŒ‡æ ‡ç»Ÿè®¡
                    category_false_positives[prediction] += 1
                    category_false_negatives[label] += 1

                    # è®°å½•é”™è¯¯åŒ¹é…
                    mismatches.append({
                        'line_number': line_num,
                        'prediction': prediction,
                        'prediction_reason': analysis['prediction_reason'],
                        'label': label,
                        'label_reason': analysis['label_reason']
                    })

                results.append(analysis)

            except Exception as e:
                print(f"å¤„ç†ç¬¬{line_num}è¡Œæ—¶å‡ºé”™: {str(e)}")
                continue

    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    overall_accuracy = correct / total if total > 0 else 0

    # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦æŒ‡æ ‡
    avg_vocab_overlap = total_vocab_overlap / total if total > 0 else 0
    avg_jaccard_sim = total_jaccard_sim / total if total > 0 else 0
    avg_cosine_sim = total_cosine_sim / total if total > 0 else 0

    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    precision_micro = precision_score(all_true_labels, all_predicted_labels, average='micro', zero_division=0)
    recall_micro = recall_score(all_true_labels, all_predicted_labels, average='micro', zero_division=0)
    f1_micro = f1_score(all_true_labels, all_predicted_labels, average='micro', zero_division=0)

    precision_macro = precision_score(all_true_labels, all_predicted_labels, average='macro', zero_division=0)
    recall_macro = recall_score(all_true_labels, all_predicted_labels, average='macro', zero_division=0)
    f1_macro = f1_score(all_true_labels, all_predicted_labels, average='macro', zero_division=0)

    precision_weighted = precision_score(all_true_labels, all_predicted_labels, average='weighted', zero_division=0)
    recall_weighted = recall_score(all_true_labels, all_predicted_labels, average='weighted', zero_division=0)
    f1_weighted = f1_score(all_true_labels, all_predicted_labels, average='weighted', zero_division=0)

    # è®¡ç®—æŒ‰ç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
    for category in all_categories:
        cat_total = categories[category]['total'] if category in categories else 0
        cat_correct = categories[category]['correct'] if category in categories else 0
        cat_errors = categories[category]['errors'] if category in categories else 0

        if category not in categories:
            categories[category] = {
                'total': 0, 'correct': 0, 'errors': 0, 'vocab_overlap': 0,
                'jaccard_sim': 0, 'cosine_sim': 0, 'error_details': []
            }

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        tp = category_true_positives[category]
        fp = category_false_positives[category]
        fn = category_false_negatives[category]

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        categories[category]['accuracy'] = cat_correct / cat_total if cat_total > 0 else 0
        categories[category]['precision'] = precision
        categories[category]['recall'] = recall
        categories[category]['f1_score'] = f1

        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦æŒ‡æ ‡
        if cat_total > 0:
            categories[category]['avg_vocab_overlap'] = categories[category]['vocab_overlap'] / cat_total
            categories[category]['avg_jaccard_sim'] = categories[category]['jaccard_sim'] / cat_total
            categories[category]['avg_cosine_sim'] = categories[category]['cosine_sim'] / cat_total
        else:
            categories[category]['avg_vocab_overlap'] = 0
            categories[category]['avg_jaccard_sim'] = 0
            categories[category]['avg_cosine_sim'] = 0

    return {
        'total_samples': total,
        'correct_predictions': correct,
        'overall_accuracy': overall_accuracy,
        'overall_accuracy_percentage': overall_accuracy * 100,
        'overall_precision_micro': precision_micro,
        'overall_recall_micro': recall_micro,
        'overall_f1_micro': f1_micro,
        'overall_precision_macro': precision_macro,
        'overall_recall_macro': recall_macro,
        'overall_f1_macro': f1_macro,
        'overall_precision_weighted': precision_weighted,
        'overall_recall_weighted': recall_weighted,
        'overall_f1_weighted': f1_weighted,
        'avg_vocab_overlap': avg_vocab_overlap,
        'avg_jaccard_sim': avg_jaccard_sim,
        'avg_cosine_sim': avg_cosine_sim,
        'category_metrics': dict(categories),
        'mismatches': mismatches,
        'detailed_results': results
    }


def analyze_file_info(filename):
    """åˆ†æå•ä¸ªæ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯"""
    info = {
        'filename': filename,
        'file_type': 'Kç‰ˆæœ¬' if '_k.' in filename else 'æ ‡å‡†ç‰ˆæœ¬',
        'extension': 'jsonl'
    }

    # ç¡®å®šæ¨¡å‹ç±»åˆ«å’Œç‰ˆæœ¬
    if 'Distill_Qwen-14B' in filename:
        info['category'] = 'Distill_Qwen-14B'
        info['model'] = 'Distill Qwen'
        info['version'] = '14B'
    elif 'llama3_8b' in filename:
        info['category'] = 'llama3_8b'
        info['model'] = 'LLaMA3'
        info['version'] = '8B'
    elif 'llama_sft' in filename:
        info['category'] = 'llama_sft'
        info['model'] = 'LLaMA SFT'
        info['version'] = '-'
    elif 'Qwen2.5-7B' in filename:
        info['category'] = 'Qwen2.5-7B'
        info['model'] = 'Qwen2.5'
        info['version'] = '7B'
    elif 'Qwen2.5-32B' in filename:
        info['category'] = 'Qwen2.5-32B'
        info['model'] = 'Qwen2.5'
        info['version'] = '32B'
    elif 'Qwen2.5-0.5B' in filename:
        info['category'] = 'Qwen2.5-0.5B'
        info['model'] = 'Qwen2.5'
        info['version'] = '0.5B'
    elif 'Qwen2.5-1.5B' in filename:
        info['category'] = 'Qwen2.5-1.5B'
        info['model'] = 'Qwen2.5'
        info['version'] = '1.5B'
    elif 'Qwen2.5-3B' in filename:
        info['category'] = 'Qwen2.5-3B'
        info['model'] = 'Qwen2.5'
        info['version'] = '3B'
    elif 'Qwen2.5-14B' in filename:
        info['category'] = 'Qwen2.5-14B'
        info['model'] = 'Qwen2.5'
        info['version'] = '14B'
    elif 'Qwen3-0.6B' in filename:
        info['category'] = 'Qwen3-0.6B'
        info['model'] = 'Qwen3'
        info['version'] = '0.6B'
    elif 'Qwen3-4B' in filename:
        info['category'] = 'Qwen3-4B'
        info['model'] = 'Qwen3'
        info['version'] = '4B'
    elif 'Qwen3-8B' in filename:
        info['category'] = 'Qwen3-8B'
        info['model'] = 'Qwen3'
        info['version'] = '8B'
    elif 'Qwen3-14B' in filename:
        info['category'] = 'Qwen3-14B'
        info['model'] = 'Qwen3'
        info['version'] = '14B'
    elif 'qwen3_sft' in filename:
        info['category'] = 'qwen3_sft'
        info['model'] = 'Qwen3 SFT'
        info['version'] = '-'
    elif 'qwen_sft' in filename:
        info['category'] = 'qwen_sft'
        info['model'] = 'Qwen SFT'
        info['version'] = '-'
    else:
        info['category'] = 'unknown'
        info['model'] = 'unknown'
        info['version'] = 'unknown'

    return info

import glob
def analyze_all_files_predictions(folder_path='.'):
    """åˆ†ææ‰€æœ‰æ–‡ä»¶çš„é¢„æµ‹ç»“æœå¹¶å¯¼å‡ºåˆ°Excel"""

    # åŠ¨æ€è·å–æ–‡ä»¶åˆ—è¡¨ï¼šæ‰€æœ‰ .json å’Œ .jsonl
    patterns = ['*.json', '*.jsonl']
    files = []
    for pattern in patterns:
        files.extend(glob.glob(os.path.join(folder_path, pattern)))
    files = sorted(files)

    # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶çš„é¢„æµ‹ç»“æœ
    all_results = []
    successful_files = []
    failed_files = []

    print("å¼€å§‹åˆ†ææ‰€æœ‰æ–‡ä»¶çš„é¢„æµ‹ç»“æœ...")
    print("=" * 80)

    for i, filename in enumerate(files, 1):
        print(f"[{i:2d}/{len(files)}] æ­£åœ¨åˆ†æ: {os.path.basename(filename)}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(filename):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            failed_files.append({'filename': filename, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨', 'status': 'failed'})
            continue

        try:
            # åˆ†æé¢„æµ‹ç»“æœ
            results = analyze_predictions_file(filename)
            file_info = analyze_file_info(filename)

            # åˆå¹¶æ–‡ä»¶ä¿¡æ¯å’Œé¢„æµ‹ç»“æœ
            combined_result = {
                'filename': filename,
                'model': file_info['model'],
                'version': file_info['version'],
                'file_type': file_info['file_type'],
                'category': file_info['category'],
                'status': 'success',
                **results
            }

            all_results.append(combined_result)
            successful_files.append(filename)

            print(f"âœ… åˆ†æå®Œæˆ - å‡†ç¡®ç‡: {results['overall_accuracy_percentage']:.2f}% "
                  f"(æ ·æœ¬æ•°: {results['total_samples']})")

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            failed_files.append({'filename': filename, 'error': str(e), 'status': 'failed'})

    print("\n" + "=" * 80)
    print(f"åˆ†æå®Œæˆ! æˆåŠŸ: {len(successful_files)}, å¤±è´¥: {len(failed_files)}")

    if not all_results:
        print("âŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„æ–‡ä»¶ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return None, []

    # åˆ›å»ºExcelæŠ¥å‘Š
    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    excel_filename = f'æ‰€æœ‰æ–‡ä»¶é¢„æµ‹ç»“æœåˆ†æ_{current_date}.xlsx'

    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:

        # å·¥ä½œè¡¨1ï¼šæ€»ä½“æ¦‚è§ˆ
        overview_data = []
        for result in all_results:
            overview_data.append([
                result['filename'],
                result['model'],
                result['version'],
                result['file_type'],
                result['total_samples'],
                result['correct_predictions'],
                f"{result['overall_accuracy_percentage']:.2f}%",
                f"{result['overall_precision_micro']:.4f}",
                f"{result['overall_recall_micro']:.4f}",
                f"{result['overall_f1_micro']:.4f}",
                f"{result['avg_vocab_overlap']:.2f}",
                f"{result['avg_jaccard_sim']:.4f}",
                f"{result['avg_cosine_sim']:.4f}"
            ])

        overview_df = pd.DataFrame(overview_data, columns=[
            'æ–‡ä»¶å', 'æ¨¡å‹', 'ç‰ˆæœ¬', 'æ–‡ä»¶ç±»å‹', 'æ€»æ ·æœ¬æ•°', 'æ­£ç¡®é¢„æµ‹æ•°', 'å‡†ç¡®ç‡',
            'å¾®å¹³å‡ç²¾ç¡®ç‡', 'å¾®å¹³å‡å¬å›ç‡', 'å¾®å¹³å‡F1', 'å¹³å‡è¯æ±‡é‡å ', 'Jaccardç›¸ä¼¼åº¦', 'ä½™å¼¦ç›¸ä¼¼åº¦'
        ])
        overview_df.to_excel(writer, sheet_name='æ€»ä½“æ¦‚è§ˆ', index=False)

        # å·¥ä½œè¡¨2ï¼šæŒ‰æ–‡ä»¶ç±»åˆ«æŒ‡æ ‡æ±‡æ€»
        # åˆ›å»ºä¸€ä¸ªä»¥æ–‡ä»¶ä¸ºè¡Œï¼Œç±»åˆ«æŒ‡æ ‡ä¸ºåˆ—çš„è¡¨æ ¼
        file_category_data = []

        # é¦–å…ˆç¡®å®šæ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
        all_categories = set()
        for result in all_results:
            all_categories.update(result['category_metrics'].keys())
        all_categories = sorted(list(all_categories))

        for result in all_results:
            row_data = [
                result['filename'],
                result['model'],
                result['file_type']
            ]

            # ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ å‡†ç¡®ç‡å’Œç›¸ä¼¼åº¦æŒ‡æ ‡
            for category in all_categories:
                if category in result['category_metrics']:
                    metrics = result['category_metrics'][category]
                    row_data.extend([
                        f"{metrics['accuracy'] * 100:.2f}%",
                        f"{metrics['avg_vocab_overlap']:.2f}",
                        f"{metrics['avg_jaccard_sim']:.4f}",
                        f"{metrics['avg_cosine_sim']:.4f}"
                    ])
                else:
                    # å¦‚æœè¯¥æ–‡ä»¶æ²¡æœ‰è¿™ä¸ªç±»åˆ«çš„æ•°æ®ï¼Œå¡«å…¥N/A
                    row_data.extend(['N/A', 'N/A', 'N/A', 'N/A'])

            file_category_data.append(row_data)

        # æ„å»ºåˆ—å
        columns = ['æ–‡ä»¶å', 'æ¨¡å‹', 'æ–‡ä»¶ç±»å‹']
        for category in all_categories:
            columns.extend([
                f'{category}_å‡†ç¡®ç‡',
                f'{category}_è¯æ±‡é‡å ',
                f'{category}_Jaccardç›¸ä¼¼åº¦',
                f'{category}_ä½™å¼¦ç›¸ä¼¼åº¦'
            ])

        file_category_df = pd.DataFrame(file_category_data, columns=columns)
        file_category_df.to_excel(writer, sheet_name='æŒ‰æ–‡ä»¶ç±»åˆ«æŒ‡æ ‡æ±‡æ€»', index=False)

        # å·¥ä½œè¡¨3ï¼šç±»åˆ«è¯¦ç»†ç»Ÿè®¡
        category_data = []
        for result in all_results:
            filename = result['filename']
            model = result['model']
            file_type = result['file_type']

            for category, metrics in result['category_metrics'].items():
                category_data.append([
                    filename,
                    model,
                    file_type,
                    category,
                    metrics['total'],
                    metrics['correct'],
                    metrics['errors'],
                    f"{metrics['accuracy'] * 100:.2f}%",
                    f"{metrics['precision']:.4f}",
                    f"{metrics['recall']:.4f}",
                    f"{metrics['f1_score']:.4f}",
                    f"{metrics['avg_vocab_overlap']:.2f}",
                    f"{metrics['avg_jaccard_sim']:.4f}",
                    f"{metrics['avg_cosine_sim']:.4f}"
                ])

        category_df = pd.DataFrame(category_data, columns=[
            'æ–‡ä»¶å', 'æ¨¡å‹', 'æ–‡ä»¶ç±»å‹', 'ç±»åˆ«', 'æ€»æ ·æœ¬æ•°', 'æ­£ç¡®é¢„æµ‹æ•°', 'é”™è¯¯é¢„æµ‹æ•°',
            'å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'å¹³å‡è¯æ±‡é‡å ', 'Jaccardç›¸ä¼¼åº¦', 'ä½™å¼¦ç›¸ä¼¼åº¦'
        ])
        category_df.to_excel(writer, sheet_name='ç±»åˆ«è¯¦ç»†ç»Ÿè®¡', index=False)

        # å·¥ä½œè¡¨4ï¼šç±»åˆ«æ±‡æ€»ç»Ÿè®¡
        # æŒ‰ç±»åˆ«æ±‡æ€»æ‰€æœ‰æ–‡ä»¶çš„ç»Ÿè®¡
        category_summary = defaultdict(lambda: {
            'total_files': 0, 'total_samples': 0, 'total_correct': 0, 'total_errors': 0,
            'accuracies': [], 'precisions': [], 'recalls': [], 'f1_scores': []
        })

        for result in all_results:
            for category, metrics in result['category_metrics'].items():
                category_summary[category]['total_files'] += 1
                category_summary[category]['total_samples'] += metrics['total']
                category_summary[category]['total_correct'] += metrics['correct']
                category_summary[category]['total_errors'] += metrics['errors']
                category_summary[category]['accuracies'].append(metrics['accuracy'])
                category_summary[category]['precisions'].append(metrics['precision'])
                category_summary[category]['recalls'].append(metrics['recall'])
                category_summary[category]['f1_scores'].append(metrics['f1_score'])

        category_summary_data = []
        for category, stats in category_summary.items():
            overall_accuracy = (stats['total_correct'] / stats['total_samples'] * 100) if stats[
                                                                                              'total_samples'] > 0 else 0
            avg_accuracy = np.mean(stats['accuracies']) * 100
            avg_precision = np.mean(stats['precisions'])
            avg_recall = np.mean(stats['recalls'])
            avg_f1 = np.mean(stats['f1_scores'])

            category_summary_data.append([
                category,
                stats['total_files'],
                stats['total_samples'],
                stats['total_correct'],
                stats['total_errors'],
                f"{overall_accuracy:.2f}%",
                f"{avg_accuracy:.2f}%",
                f"{avg_precision:.4f}",
                f"{avg_recall:.4f}",
                f"{avg_f1:.4f}"
            ])

        category_summary_df = pd.DataFrame(category_summary_data, columns=[
            'ç±»åˆ«', 'æ¶‰åŠæ–‡ä»¶æ•°', 'æ€»æ ·æœ¬æ•°', 'æ€»æ­£ç¡®æ•°', 'æ€»é”™è¯¯æ•°',
            'æ•´ä½“å‡†ç¡®ç‡', 'å¹³å‡å‡†ç¡®ç‡', 'å¹³å‡ç²¾ç¡®ç‡', 'å¹³å‡å¬å›ç‡', 'å¹³å‡F1åˆ†æ•°'
        ])
        category_summary_df.to_excel(writer, sheet_name='ç±»åˆ«æ±‡æ€»ç»Ÿè®¡', index=False)

        # å·¥ä½œè¡¨5ï¼šè¯¦ç»†è¯„ä¼°æŒ‡æ ‡
        detailed_data = []
        for result in all_results:
            detailed_data.append([
                result['filename'],
                result['model'],
                result['total_samples'],
                f"{result['overall_accuracy_percentage']:.2f}%",
                f"{result['overall_precision_micro']:.4f}",
                f"{result['overall_recall_micro']:.4f}",
                f"{result['overall_f1_micro']:.4f}",
                f"{result['overall_precision_macro']:.4f}",
                f"{result['overall_recall_macro']:.4f}",
                f"{result['overall_f1_macro']:.4f}",
                f"{result['overall_precision_weighted']:.4f}",
                f"{result['overall_recall_weighted']:.4f}",
                f"{result['overall_f1_weighted']:.4f}"
            ])

        detailed_df = pd.DataFrame(detailed_data, columns=[
            'æ–‡ä»¶å', 'æ¨¡å‹', 'æ€»æ ·æœ¬æ•°', 'å‡†ç¡®ç‡',
            'å¾®å¹³å‡ç²¾ç¡®ç‡', 'å¾®å¹³å‡å¬å›ç‡', 'å¾®å¹³å‡F1',
            'å®å¹³å‡ç²¾ç¡®ç‡', 'å®å¹³å‡å¬å›ç‡', 'å®å¹³å‡F1',
            'åŠ æƒå¹³å‡ç²¾ç¡®ç‡', 'åŠ æƒå¹³å‡å¬å›ç‡', 'åŠ æƒå¹³å‡F1'
        ])
        detailed_df.to_excel(writer, sheet_name='è¯¦ç»†è¯„ä¼°æŒ‡æ ‡', index=False)

        # å·¥ä½œè¡¨6ï¼šæŒ‰æ¨¡å‹ç±»å‹æ±‡æ€»
        model_summary = defaultdict(lambda: {
            'files': [], 'total_samples': 0, 'total_correct': 0,
            'accuracies': [], 'f1_scores': []
        })

        for result in all_results:
            model = result['model']
            model_summary[model]['files'].append(result['filename'])
            model_summary[model]['total_samples'] += result['total_samples']
            model_summary[model]['total_correct'] += result['correct_predictions']
            model_summary[model]['accuracies'].append(result['overall_accuracy_percentage'])
            model_summary[model]['f1_scores'].append(result['overall_f1_micro'])

        model_data = []
        for model, stats in model_summary.items():
            avg_accuracy = np.mean(stats['accuracies'])
            avg_f1 = np.mean(stats['f1_scores'])
            overall_accuracy = (stats['total_correct'] / stats['total_samples'] * 100) if stats[
                                                                                              'total_samples'] > 0 else 0

            model_data.append([
                model,
                len(stats['files']),
                stats['total_samples'],
                stats['total_correct'],
                f"{overall_accuracy:.2f}%",
                f"{avg_accuracy:.2f}%",
                f"{avg_f1:.4f}",
                '; '.join(stats['files'])
            ])

        model_df = pd.DataFrame(model_data, columns=[
            'æ¨¡å‹', 'æ–‡ä»¶æ•°', 'æ€»æ ·æœ¬æ•°', 'æ€»æ­£ç¡®æ•°', 'æ•´ä½“å‡†ç¡®ç‡', 'å¹³å‡å‡†ç¡®ç‡', 'å¹³å‡F1', 'æ–‡ä»¶åˆ—è¡¨'
        ])
        model_df.to_excel(writer, sheet_name='æŒ‰æ¨¡å‹æ±‡æ€»', index=False)

        # å·¥ä½œè¡¨7ï¼šæ–‡ä»¶ç±»å‹å¯¹æ¯” (æ ‡å‡†ç‰ˆæœ¬ vs Kç‰ˆæœ¬)
        type_comparison = defaultdict(lambda: {
            'files': [], 'total_samples': 0, 'total_correct': 0,
            'accuracies': [], 'f1_scores': []
        })

        for result in all_results:
            file_type = result['file_type']
            type_comparison[file_type]['files'].append(result['filename'])
            type_comparison[file_type]['total_samples'] += result['total_samples']
            type_comparison[file_type]['total_correct'] += result['correct_predictions']
            type_comparison[file_type]['accuracies'].append(result['overall_accuracy_percentage'])
            type_comparison[file_type]['f1_scores'].append(result['overall_f1_micro'])

        type_data = []
        for file_type, stats in type_comparison.items():
            avg_accuracy = np.mean(stats['accuracies'])
            avg_f1 = np.mean(stats['f1_scores'])
            overall_accuracy = (stats['total_correct'] / stats['total_samples'] * 100) if stats[
                                                                                              'total_samples'] > 0 else 0

            type_data.append([
                file_type,
                len(stats['files']),
                stats['total_samples'],
                stats['total_correct'],
                f"{overall_accuracy:.2f}%",
                f"{avg_accuracy:.2f}%",
                f"{avg_f1:.4f}"
            ])

        type_df = pd.DataFrame(type_data, columns=[
            'æ–‡ä»¶ç±»å‹', 'æ–‡ä»¶æ•°', 'æ€»æ ·æœ¬æ•°', 'æ€»æ­£ç¡®æ•°', 'æ•´ä½“å‡†ç¡®ç‡', 'å¹³å‡å‡†ç¡®ç‡', 'å¹³å‡F1'
        ])
        type_df.to_excel(writer, sheet_name='æ–‡ä»¶ç±»å‹å¯¹æ¯”', index=False)

        # å·¥ä½œè¡¨8ï¼šé”™è¯¯åŒ¹é…è¯¦æƒ…ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if any('mismatches' in result and result['mismatches'] for result in all_results):
            mismatch_data = []
            for result in all_results:
                if 'mismatches' in result and result['mismatches']:
                    for mismatch in result['mismatches']:
                        mismatch_data.append([
                            result['filename'],
                            mismatch['line_number'],
                            mismatch['prediction'],
                            mismatch['label'],
                            mismatch.get('prediction_reason', '')[:100] + "..." if len(
                                mismatch.get('prediction_reason', '')) > 100 else mismatch.get('prediction_reason', ''),
                            mismatch.get('label_reason', '')[:100] + "..." if len(
                                mismatch.get('label_reason', '')) > 100 else mismatch.get('label_reason', '')
                        ])

            if mismatch_data:
                mismatch_df = pd.DataFrame(mismatch_data, columns=[
                    'æ–‡ä»¶å', 'è¡Œå·', 'é¢„æµ‹ç»“æœ', 'çœŸå®æ ‡ç­¾', 'é¢„æµ‹ç†ç”±', 'æ ‡ç­¾ç†ç”±'
                ])
                mismatch_df.to_excel(writer, sheet_name='é”™è¯¯åŒ¹é…è¯¦æƒ…', index=False)

        # å·¥ä½œè¡¨9ï¼šå¤±è´¥æ–‡ä»¶åˆ—è¡¨
        if failed_files:
            failed_df = pd.DataFrame(failed_files)
            failed_df.to_excel(writer, sheet_name='å¤±è´¥æ–‡ä»¶', index=False)

    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print("\n" + "ğŸ† æœ€ä½³è¡¨ç°æ–‡ä»¶ (æŒ‰å‡†ç¡®ç‡æ’åº)")
    print("-" * 80)
    sorted_results = sorted(all_results, key=lambda x: x['overall_accuracy_percentage'], reverse=True)
    for i, result in enumerate(sorted_results[:10], 1):
        print(f"{i:2d}. {result['filename']:<35} "
              f"å‡†ç¡®ç‡: {result['overall_accuracy_percentage']:6.2f}% "
              f"F1: {result['overall_f1_micro']:.4f} "
              f"æ ·æœ¬: {result['total_samples']:4d}")

    print(f"\nâœ… è¯¦ç»†ExcelæŠ¥å‘Šå·²ä¿å­˜: {excel_filename}")

    return excel_filename, all_results


def print_overall_summary(all_results):
    """æ‰“å°æ•´ä½“ç»Ÿè®¡æ‘˜è¦"""
    if not all_results:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•æ–‡ä»¶")
        return

    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ•´ä½“ç»Ÿè®¡æ‘˜è¦")
    print("=" * 80)

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    total_files_analyzed = len(all_results)
    total_samples = sum(r['total_samples'] for r in all_results)
    total_correct = sum(r['correct_predictions'] for r in all_results)
    overall_accuracy = (total_correct / total_samples * 100) if total_samples > 0 else 0

    avg_accuracy = np.mean([r['overall_accuracy_percentage'] for r in all_results])
    avg_f1 = np.mean([r['overall_f1_micro'] for r in all_results])

    print(f"æˆåŠŸåˆ†ææ–‡ä»¶æ•°: {total_files_analyzed}")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"æ€»æ­£ç¡®é¢„æµ‹æ•°: {total_correct:,}")
    print(f"æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.2f}%")
    print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")
    print(f"å¹³å‡F1åˆ†æ•°: {avg_f1:.4f}")

    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡:")
    print("-" * 60)
    category_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'errors': 0})

    for result in all_results:
        for category, metrics in result['category_metrics'].items():
            category_stats[category]['total'] += metrics['total']
            category_stats[category]['correct'] += metrics['correct']
            category_stats[category]['errors'] += metrics['errors']

    for category, stats in sorted(category_stats.items()):
        accuracy = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0
        print(f"{category:<8}: æ€»æ ·æœ¬ {stats['total']:5d}, æ­£ç¡® {stats['correct']:5d}, "
              f"é”™è¯¯ {stats['errors']:4d}, å‡†ç¡®ç‡ {accuracy:6.2f}%")

    # æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡:")
    print("-" * 50)
    model_stats = defaultdict(lambda: {'files': 0, 'samples': 0, 'correct': 0, 'accuracies': []})

    for result in all_results:
        model = result['model']
        model_stats[model]['files'] += 1
        model_stats[model]['samples'] += result['total_samples']
        model_stats[model]['correct'] += result['correct_predictions']
        model_stats[model]['accuracies'].append(result['overall_accuracy_percentage'])

    for model, stats in sorted(model_stats.items()):
        overall_acc = (stats['correct'] / stats['samples'] * 100) if stats['samples'] > 0 else 0
        avg_acc = np.mean(stats['accuracies'])
        print(f"{model:<15}: {stats['files']} æ–‡ä»¶, æ•´ä½“å‡†ç¡®ç‡: {overall_acc:6.2f}%, å¹³å‡å‡†ç¡®ç‡: {avg_acc:6.2f}%")

    # æŒ‰æ–‡ä»¶ç±»å‹å¯¹æ¯” (æ ‡å‡†ç‰ˆæœ¬ vs Kç‰ˆæœ¬)
    print("\nğŸ”„ æ ‡å‡†ç‰ˆæœ¬ vs Kç‰ˆæœ¬å¯¹æ¯”:")
    print("-" * 40)
    type_stats = defaultdict(lambda: {'files': 0, 'samples': 0, 'correct': 0, 'accuracies': []})

    for result in all_results:
        file_type = result['file_type']
        type_stats[file_type]['files'] += 1
        type_stats[file_type]['samples'] += result['total_samples']
        type_stats[file_type]['correct'] += result['correct_predictions']
        type_stats[file_type]['accuracies'].append(result['overall_accuracy_percentage'])

    for file_type, stats in sorted(type_stats.items()):
        overall_acc = (stats['correct'] / stats['samples'] * 100) if stats['samples'] > 0 else 0
        avg_acc = np.mean(stats['accuracies'])
        print(f"{file_type:<10}: {stats['files']} æ–‡ä»¶, æ•´ä½“å‡†ç¡®ç‡: {overall_acc:6.2f}%, å¹³å‡å‡†ç¡®ç‡: {avg_acc:6.2f}%")


def save_category_metrics_csv(all_results):
    """ä¿å­˜æ¯ä¸ªæ–‡ä»¶æŒ‰ç±»åˆ«çš„æŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    csv_filename = f'æŒ‰æ–‡ä»¶ç±»åˆ«æŒ‡æ ‡è¯¦æƒ…_{current_date}.csv'

    # é¦–å…ˆç¡®å®šæ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
    all_categories = set()
    for result in all_results:
        all_categories.update(result['category_metrics'].keys())
    all_categories = sorted(list(all_categories))

    csv_data = []
    for result in all_results:
        for category in all_categories:
            if category in result['category_metrics']:
                metrics = result['category_metrics'][category]
                csv_data.append([
                    result['filename'],
                    result['model'],
                    result['version'],
                    result['file_type'],
                    category,
                    metrics['total'],
                    metrics['correct'],
                    metrics['errors'],
                    f"{metrics['accuracy'] * 100:.2f}",
                    f"{metrics['avg_vocab_overlap']:.2f}",
                    f"{metrics['avg_jaccard_sim']:.4f}",
                    f"{metrics['avg_cosine_sim']:.4f}",
                    f"{metrics['precision']:.4f}",
                    f"{metrics['recall']:.4f}",
                    f"{metrics['f1_score']:.4f}"
                ])

    # ä¿å­˜ä¸ºCSV
    csv_df = pd.DataFrame(csv_data, columns=[
        'æ–‡ä»¶å', 'æ¨¡å‹', 'ç‰ˆæœ¬', 'æ–‡ä»¶ç±»å‹', 'ç±»åˆ«',
        'æ€»æ ·æœ¬æ•°', 'æ­£ç¡®é¢„æµ‹æ•°', 'é”™è¯¯é¢„æµ‹æ•°', 'å‡†ç¡®ç‡(%)',
        'å¹³å‡è¯æ±‡é‡å ', 'Jaccardç›¸ä¼¼åº¦', 'ä½™å¼¦ç›¸ä¼¼åº¦',
        'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°'
    ])

    csv_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')

    print(f"âœ… æŒ‰ç±»åˆ«æŒ‡æ ‡è¯¦æƒ…å·²ä¿å­˜è‡³CSVæ–‡ä»¶: {csv_filename}")
    return csv_filename


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ä»¶çš„é¢„æµ‹ç»“æœ...")

    # åˆ†ææ‰€æœ‰æ–‡ä»¶çš„é¢„æµ‹ç»“æœ
    excel_file, all_results = analyze_all_files_predictions()

    # ä¿å­˜æŒ‰ç±»åˆ«æŒ‡æ ‡çš„CSVæ–‡ä»¶
    if all_results:
        csv_file = save_category_metrics_csv(all_results)

    # æ‰“å°æ•´ä½“ç»Ÿè®¡æ‘˜è¦
    print_overall_summary(all_results)

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ç”Ÿæˆçš„Excelå’ŒCSVæ–‡ä»¶ã€‚")
    print("=" * 80)